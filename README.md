# Prompt Diff â€” Behavioral Prompt Analyzer

Prompt Diff is a lightweight web tool that analyzes how changes between two prompts
impact an LLMâ€™s behavior. Instead of focusing on surface-level text differences,
it highlights _meaningful behavioral shifts_ that affect cost, reliability, and
model safety.

ğŸ‘‰ **Live Demo:** https://your-render-url-here  
ğŸ‘‰ Built with **FastAPI**, **Jinja2**, and a clean minimal UI.

---

## âœ¨ Features

- ğŸ” **Behavioral Impact Analysis**  
  Detects how prompt edits change model constraints, certainty, and tone.

- ğŸ’¬ **Risk Score (0â€“10)**  
  A heuristic scoring system that captures common LLM failure modes such as  
  under-specification, overconfidence, missing roles, or weak constraints.

- ğŸ§  **Risk Banding**  
  Labels prompts as **Low**, **Moderate**, or **High** risk depending on their
  likelihood of causing drift or hallucination.

- ğŸ“‰ **Token Delta Reporting**  
  Shows how edits affect cost + latency.

- ğŸ¨ **Clean Web UI**  
  A sleek, distraction-free interface for fast comparisons.

---

## ğŸ–¼ï¸ UI Preview

_(Add a screenshot here after deployment)_

---

## ğŸš€ Running Locally

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

2. Start the server
   uvicorn app:app --reload

3. Visit the app
   http://127.0.0.1:8000

ğŸ—‚ï¸ Project Structure
prompt-diff/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ index.html
â””â”€â”€ static/
â””â”€â”€ style.css

ğŸ’¡ Why I Built This

Prompt engineering tools often treat prompts as plain text.
Prompt Diff treats prompts like code â€” where small changes matter.

This tool helps you:

Design safer prompts

Reduce hallucination risk

Understand how constraints guide AI behavior

Iterate prompts with intention

ğŸ“„ License

MIT
